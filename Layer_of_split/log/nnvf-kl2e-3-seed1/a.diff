diff --git a/Layer_of_split/dualCopy.py b/Layer_of_split/dualCopy.py
index 26be569..12c4209 100644
--- a/Layer_of_split/dualCopy.py
+++ b/Layer_of_split/dualCopy.py
@@ -5,7 +5,7 @@ import logz
 import scipy.signal
 import tensorflow as tf
 import matplotlib
-matplotlib.use('Agg')
+#matplotlib.use('Agg')
 import matplotlib.pyplot as plt 
 tiny = 1e-10
 def normc_initializer(std=1.0):
@@ -292,7 +292,7 @@ def main_cartpole_split(numBatches=50, gamma=1.0, min_timesteps_per_batch=1000,
     
     return MeanRewardHistory
    
-def main_cartpole_vpg(numBatches = 50, gamma=1.0, min_timesteps_per_batch=1000, stepsize=1e-2, animate=True, logdir=None, vf_type='linear'):                                            
+def cartpoleVpg(numBatches = 50, gamma=1.0, min_timesteps_per_batch=1000, stepsize=1e-2, animate=True, logdir=None, vf_type='linear'):                                            
      tf.reset_default_graph()                                                                                                                                                           
      env = gym.make("CartPole-v0")                                                                                                                                                      
      ob_dim = env.observation_space.shape[0]                                                                                                                                            
@@ -470,7 +470,7 @@ def main_cartpole_vpg(numBatches = 50, gamma=1.0, min_timesteps_per_batch=1000,
          # Note that we fit value function AFTER using it to compute the advantage function to avoid introducing bias
          logz.dump_tabular()
      
-     return MeanRewardHistory
+     return MeanRewardHistory, _, PG_Loss
 
 
 
@@ -482,7 +482,7 @@ def main_cartpole_vpg(numBatches = 50, gamma=1.0, min_timesteps_per_batch=1000,
     
 def run(case):
     if case == 0 or case < 0:
-        batches = 50
+        batches = 1
         MeanSplitRewards = main_cartpole_split(numBatches = batches,logdir=None,vf_type='linear', animate=False) # when you want to start collecting results, set the logdir
         MeanVpgRewards = main_cartpole_vpg(numBatches = batches, logdir=None,vf_type = 'linear', animate=False)
 
diff --git a/Layer_of_split/last1_Vpg.py b/Layer_of_split/last1_Vpg.py
index c92b2e6..cfae5c7 100644
--- a/Layer_of_split/last1_Vpg.py
+++ b/Layer_of_split/last1_Vpg.py
@@ -138,7 +138,7 @@ def normal_kl(old_mean, old_log_std, new_mean, new_log_std):
         numerator / denominator + new_log_std - old_log_std, axis=1)
 def normal_entropy(log_std):
     return tf.reduce_sum(log_std + np.log(np.sqrt(2 * np.pi * np.e)), axis=1)
-def main_cartpole_split(numBatches=50, gamma=1.0, min_timesteps_per_batch=1000, stepsize=1e-2, animate=True, logdir=None, vf_type='linear'):
+def cartpoleSplit_1(numBatches=50, gamma=1.0, min_timesteps_per_batch=1000, stepsize=1e-2, animate=True, logdir=None, vf_type='linear'):
     tf.reset_default_graph()
     env = gym.make("CartPole-v0")
     ob_dim = env.observation_space.shape[0]
@@ -156,17 +156,19 @@ def main_cartpole_split(numBatches=50, gamma=1.0, min_timesteps_per_batch=1000,
     with tf.variable_scope("SL_vars"):
         sy_h1 = lrelu(dense(sy_ob_no, 32, "h1", weight_init=normc_initializer(1.0))) # hidden layer
         sy_h2 = lrelu(dense(sy_h1, 32, "h2", weight_init=normc_initializer(1.0))) # hidden layer
-        critical_layer = lrelu(dense(sy_h2, 16, "criticalLayer", weight_init=normc_initializer(1.0))) # hidden layer
+        sy_h3 = lrelu(dense(sy_h2, 32, "h3", weight_init=normc_initializer(1.0))) # hidden layen      
+        critical_layer = dense(sy_h3, 16, "criticalLayer", weight_init=normc_initializer(1.0)) # hidden layer
+
     with tf.variable_scope("PG_vars"):
         #The weights of the two following layers are in the PG scope
-        sy_h3 = lrelu(dense(critical_layer, 32, "h3", weight_init=normc_initializer(1.0))) # hidden layen
-        sy_logits_na = dense(sy_h3, num_actions, "final", weight_init=normc_initializer(0.05))
+
+        sy_logits_na = dense(critical_layer, num_actions, "final", weight_init=normc_initializer(0.05))
         
         activation = tf.Variable(initial_value = tf.zeros([numTimestepsBatch,16], dtype = tf.float32),name = "activation", trainable = True)
         activationOp = activation.assign(critical_layer)
     with tf.variable_scope("PG_vars", reuse = True):
-        h3_back = lrelu(dense(activation, 32, "h3", weight_init=normc_initializer(1.0))) # hidden layer
-        sy_logits_back = dense(h3_back, num_actions, "final", weight_init=normc_initializer(0.05))
+        #h3_back = lrelu(dense(activation, 32, "h3", weight_init=normc_initializer(1.0))) # hidden layer
+        sy_logits_back = dense(activation, num_actions, "final", weight_init=normc_initializer(0.05))
     
     
     
@@ -290,7 +292,7 @@ def main_cartpole_split(numBatches=50, gamma=1.0, min_timesteps_per_batch=1000,
         # Note that we fit value function AFTER using it to compute the advantage function to avoid introducing bias
         logz.dump_tabular()
     
-    return MeanRewardHistory
+    return MeanRewardHistory , SL_Loss, PG_Loss
    
  
     
diff --git a/Layer_of_split/last2_Vpg.py b/Layer_of_split/last2_Vpg.py
index 217973b..5addbba 100644
--- a/Layer_of_split/last2_Vpg.py
+++ b/Layer_of_split/last2_Vpg.py
@@ -138,7 +138,7 @@ def normal_kl(old_mean, old_log_std, new_mean, new_log_std):
         numerator / denominator + new_log_std - old_log_std, axis=1)
 def normal_entropy(log_std):
     return tf.reduce_sum(log_std + np.log(np.sqrt(2 * np.pi * np.e)), axis=1)
-def main_cartpole_split(numBatches=50, gamma=1.0, min_timesteps_per_batch=1000, stepsize=1e-2, animate=True, logdir=None, vf_type='linear'):
+def cartpoleSplit_2(numBatches=50, gamma=1.0, min_timesteps_per_batch=1000, stepsize=1e-2, animate=True, logdir=None, vf_type='linear'):
     tf.reset_default_graph()
     env = gym.make("CartPole-v0")
     ob_dim = env.observation_space.shape[0]
@@ -156,19 +156,19 @@ def main_cartpole_split(numBatches=50, gamma=1.0, min_timesteps_per_batch=1000,
     with tf.variable_scope("SL_vars"):
         sy_h1 = lrelu(dense(sy_ob_no, 32, "h1", weight_init=normc_initializer(1.0))) # hidden layer
         sy_h2 = lrelu(dense(sy_h1, 32, "h2", weight_init=normc_initializer(1.0))) # hidden layer
-        sy_h3 = lrelu(dense(sy_h2 32, "h3", weight_init=normc_initializer(1.0))) # hidden layen      
-        critical_layer = dense(sy_h3, 16, "criticalLayer", weight_init=normc_initializer(1.0)) # hidden layer
+        critical_layer = dense(sy_h2, 16, "criticalLayer", weight_init=normc_initializer(1.0)) # hidden layer
 
     with tf.variable_scope("PG_vars"):
         #The weights of the two following layers are in the PG scope
-
-        sy_logits_na = dense(critical_layer, num_actions, "final", weight_init=normc_initializer(0.05))
+        sy_h3 = lrelu(dense(critical_layer, 32, "h3", weight_init=normc_initializer(1.0))) # hidden layen
+        sy_logits_na = dense(sy_h3, num_actions, "final", weight_init=normc_initializer(0.05))
         
         activation = tf.Variable(initial_value = tf.zeros([numTimestepsBatch,16], dtype = tf.float32),name = "activation", trainable = True)
         activationOp = activation.assign(critical_layer)
+
     with tf.variable_scope("PG_vars", reuse = True):
-        #h3_back = lrelu(dense(activation, 32, "h3", weight_init=normc_initializer(1.0))) # hidden layer
-        sy_logits_back = dense(activation, num_actions, "final", weight_init=normc_initializer(0.05))
+        h3_back = lrelu(dense(activation, 32, "h3", weight_init=normc_initializer(1.0))) # hidden layer
+        sy_logits_back = dense(h3_back, num_actions, "final", weight_init=normc_initializer(0.05))
     
     
     
@@ -292,7 +292,7 @@ def main_cartpole_split(numBatches=50, gamma=1.0, min_timesteps_per_batch=1000,
         # Note that we fit value function AFTER using it to compute the advantage function to avoid introducing bias
         logz.dump_tabular()
     
-    return MeanRewardHistory
+    return MeanRewardHistory, SL_Loss, PG_Loss
    
  
     
diff --git a/Layer_of_split/last3_Vpg.py b/Layer_of_split/last3_Vpg.py
index 2cd2830..547caa6 100644
--- a/Layer_of_split/last3_Vpg.py
+++ b/Layer_of_split/last3_Vpg.py
@@ -138,7 +138,7 @@ def normal_kl(old_mean, old_log_std, new_mean, new_log_std):
         numerator / denominator + new_log_std - old_log_std, axis=1)
 def normal_entropy(log_std):
     return tf.reduce_sum(log_std + np.log(np.sqrt(2 * np.pi * np.e)), axis=1)
-def main_cartpole_split(numBatches=50, gamma=1.0, min_timesteps_per_batch=1000, stepsize=1e-2, animate=True, logdir=None, vf_type='linear'):
+def cartpoleSplit_3(numBatches=50, gamma=1.0, min_timesteps_per_batch=1000, stepsize=1e-2, animate=True, logdir=None, vf_type='linear'):
     tf.reset_default_graph()
     env = gym.make("CartPole-v0")
     ob_dim = env.observation_space.shape[0]
@@ -295,7 +295,7 @@ def main_cartpole_split(numBatches=50, gamma=1.0, min_timesteps_per_batch=1000,
         # Note that we fit value function AFTER using it to compute the advantage function to avoid introducing bias
         logz.dump_tabular()
     
-    return MeanRewardHistory
+    return MeanRewardHistory, SL_Loss, PG_Loss
    
  
     
diff --git a/Layer_of_split/last4_Vpg.py b/Layer_of_split/last4_Vpg.py
index 1cfa17a..afc0ab8 100644
--- a/Layer_of_split/last4_Vpg.py
+++ b/Layer_of_split/last4_Vpg.py
@@ -138,7 +138,7 @@ def normal_kl(old_mean, old_log_std, new_mean, new_log_std):
         numerator / denominator + new_log_std - old_log_std, axis=1)
 def normal_entropy(log_std):
     return tf.reduce_sum(log_std + np.log(np.sqrt(2 * np.pi * np.e)), axis=1)
-def main_cartpole_split(numBatches=50, gamma=1.0, min_timesteps_per_batch=1000, stepsize=1e-2, animate=True, logdir=None, vf_type='linear'):
+def cartpoleSplit_4(numBatches=50, gamma=1.0, min_timesteps_per_batch=1000, stepsize=1e-2, animate=True, logdir=None, vf_type='linear'):
     tf.reset_default_graph()
     env = gym.make("CartPole-v0")
     ob_dim = env.observation_space.shape[0]
@@ -296,7 +296,7 @@ def main_cartpole_split(numBatches=50, gamma=1.0, min_timesteps_per_batch=1000,
         # Note that we fit value function AFTER using it to compute the advantage function to avoid introducing bias
         logz.dump_tabular()
     
-    return MeanRewardHistory
+    return MeanRewardHistory, SL_Loss, PG_Loss
    
  
     
diff --git a/dualCopy.py b/dualCopy.py
index 26be569..7ac0546 100644
--- a/dualCopy.py
+++ b/dualCopy.py
@@ -470,7 +470,7 @@ def main_cartpole_vpg(numBatches = 50, gamma=1.0, min_timesteps_per_batch=1000,
          # Note that we fit value function AFTER using it to compute the advantage function to avoid introducing bias
          logz.dump_tabular()
      
-     return MeanRewardHistory
+     return MeanRewardHistory, _, PG_Loss
 
 
 
diff --git a/out.png b/out.png
deleted file mode 100644
index f7981bd..0000000
Binary files a/out.png and /dev/null differ
